\documentclass{stdlocal}
\begin{document}
\section{Background} % (fold)
\label{sec:background}
  % virtual subsection Introduction
  To systematically approach the implementation of PRNGs, basic knowledge in the topics of stochastics and finite fields is administrable.
  Together, these topics will give a deeper understanding of randomness in deterministic computer systems, a formal description of pseudorandom sequences and generators, and the mathematical foundation of Monte Carlo algorithms.
  Based on them, we are capable of scientifically analyzing PRNGs concerning their randomness properties.
  Vectorization techniques can be conceptualized by the architecture of modern SIMD-capable multiprocessors and their instruction sets.
  Especially the knowledge of typical instructions will make the design of a new API and its application to Monte Carlo simulations clear.
  The following sections will give an overview of the named topics.

  \subsection{Mathematical Preliminaries} % (fold)
  \label{sub:mathematical_preliminaries}
    \subsubsection*{Probability Theory} % (fold)
    \label{ssub:stochastics}
      The observation of random experiments resulted in the construction of probability theory.
      But probability theory itself does not use a further formalized concept of randomness \autocite{schmidt2009}.
      % Randomness itself plays a minor role in probability theory and is used in form of realizations of random variables.
      In fact, it allows us to observe randomness without defining it \autocite{volchan2002}.
      % Actually, typical formalizations rely on probability theory.
      % This connection makes the development of RNGs possible.
      % Hence, in the following we will give only the formal definition of relevant structures without further discussions and will postpone an examination of randomness to the next subsection.
      Hence, we will postpone an examination of truly random sequences to the next section.

      According to \textcite{schmidt2009}, Kolmogorov embedded probability theory in the theory of measure and integration.
      Albeit it heavily relies on measure-theoretical structures, probability theory is one of the most important applications of measure and integration theory.
      Therefore we will assume basic knowledge in this topic and refer to \textcite{schmidt2009} and \textcite{elstrodt2011} for a more detailed introduction to measure spaces, measurable functions, and integrals.
      Propositions and theorems will be given without proof.

      The underlying structure of probability theory, which connects it to measure theory, is the probability space.
      It is a measure space with a finite and normalized measure.
      This gives access to all the usual results of measure theory and furthermore unifies discrete and continuous distributions.
      \autocite[p.~193~ff.]{schmidt2009}

      \begin{definition}[Probability Space]
        A probability space is a measure space $\roundBrackets{\Omega, \mathscr{F}, P}$ such that $P(\Omega)=1$.
        In this case, we call $P$ the probability measure, $\mathscr{F}$ the set of all events, and $\Omega$ the set of all possible outcomes of a random experiment.
      \end{definition}

      % For our purposes, the set of possible outcomes $\Omega$ will be a finite or countable infinite set.
      % Hence, we can choose $\mathscr{F}$ to be the power set $\mathscr{P}(\Omega)$.
      Due to the complex definition of a measure space, it is convenient to not have to explicitly specify the probability space when analyzing random experiments.
      Instead, we use random variables which are essentially measurable functions on a probability space \autocite[p.~194]{schmidt2009}.
      For complicated cases, these will serve as observables for specific properties and will make the analysis much more intuitive.

      \begin{definition}[Random Variable]
        Let $(\Omega,\mathscr{F},P)$ be a probability space and $(\Sigma,\mathscr{A})$ a measurable space.
        A measurable function $\function{X}{\Omega}{\Sigma}$ is called a random variable.

        In this case, we denote with $P_X\define P\composition\inverse{X}$ the distribution and with $(\Sigma,\mathscr{A},P_X)$ the probability space of $X$.
        % We call $X(ω)$ for $ω\in\Omega$ a realization of $X$.
        % Let $\function{Y}{\Omega}{\Sigma}$ be another random variable.
        % $X$ and $Y$ are identically distributed if $P_X = P_Y$.
        Two random variables are identically distributed if they have the same distribution.
        Additionally, we say that $X$ is a real-valued random variable if $\Sigma = \setReal$ and $\mathscr{A} = \mathscr{B}(\setReal)$.
      \end{definition}

      From now on, if a random variable is defined then, if not stated otherwise, it is assumed there exists a proper probability space $(\Omega,\mathscr{F},P)$ and measurable space $(\Sigma, \mathscr{A})$.

      Another important concept of stochastics is known as independence.
      In \textcite{schmidt2009} it is defined for a family of events, a family of sets of events, and a family of random variables.
      If we think of random variables as observables then their independence means that their outcomes do not influence each other.
      % It makes only sense in the context of probability theory
      For our purposes, the general definition of all three forms of independence is distracting.
      In a computer, it makes no sense to talk about infinite sequences.
      Therefore the following definition of independence takes only a finite sequence of random variables into account.
      Furthermore, to make it more understandable, this definition uses a theorem from \textcite[p.~238]{schmidt2009} which characterizes the independence of random variables.
      % Here, we will use a simpler equivalent definition  because, for a computer, all we need are finite sequences of random variables.

      \begin{definition}[Independence]
        % Let $(\Omega,\mathscr{F},P)$ be a probability space.
        % Two events $A, B \in \mathscr{F}$ are independent if $P(A\cap B)=P(A)P(B)$.
        % Let $(\Sigma_i,\mathscr{A}_i)$ for $i\in\set{1,2}{}$ be measurable spaces and $X_i$ random variables with $X\define X_1\times X_2$.
        % They are called independent if $P_X = P_{X_1} \otimes P_{X_2}$.
        Let $n\in\setNatural$ and $X_i$ be a random variable for all $i\in\setNatural$ with $i\leq n$.
        We denote the respective random vector with $X \define \roundBrackets{X_i}_{i=1}^n$.
        Then these random variables are independent if the following equation holds.
        \[
          P_X = \bigotimes_{i=1}^n P_{X_i}
        \]
      \end{definition}

      Typical observations of random sequences include the estimation of the expectation value and the variance.
      Both of these values are needed for analyzing PRNGs and the development of Monte Carlo simulations \autocite[p.~30~ff.]{landau2015}.
      Due to their deep connection to the integral, both of these moments are defined for real-valued random variables.
      We give the usual definitions based on \textcite[p.~274~ff.]{schmidt2009} in a simplified form.

      \begin{definition}[Expectation Value and Variance]
        Let $X$ be a real-valued random variable such that $\integral{\Omega}{}{\absolute{X}}{P}<\infty$.
        Then the expectation value $\expect X$ and variance $\var X$ of $X$ is defined in the following way.
        \[
          \expect X \define \integral{\Omega}{}{X(ω)}{P(ω)}
          \separate
          \var X \define \expect\roundBrackets{X - \expect X}^2
        \]
      \end{definition}

      To not rely on the underlying probability space directly, we want to be able to compute the expectation value through the respective distribution of the random variable.
      The theory of measure and integration gives the following proposition, also known as rule of substitution \autocite[p.~276]{schmidt2009}.

      \begin{proposition}[Substitution]
        Let $X$ be real-valued random variable and $\function{f}{\setReal}{\setReal}$ a measurable function such that $\integral{\Omega}{}{\absolute{f}}{P_X} < \infty$.
        Then the following equation holds.
        \[
          \expect(f\composition X) = \integral{\setReal}{}{f(x)}{P_X(x)}
        \]
        In particular, if $\expect \absolute{X} < \infty$ then the above equation can be reformulated as follows.
        \[
          \expect X = \integral{\setReal}{}{x}{P_X(x)}
        \]
      \end{proposition}

      The distribution of real-valued random variables is univariate and as a result can be described by so-called cumulative distribution functions (CDFs).
      The CDF intuitively characterizes the distribution and simplifies the analysis.
      Further, it can be proven that every CDF belongs to a univariate distribution.
      According to \textcite[p.~246]{schmidt2009}, this is the theorem of correspondence.
      Sometimes it is even possible to define a probability density; a function that is the Lebesgue density of the respective distribution \autocite[p.~255]{schmidt2009}.

      \begin{definition}[Probability Density and Cumulative Distribution Function]
        Let $X$ be a real-valued random variable.
        Then the respective cumulative distribution function is defined as follows.
        \[
          \function{F_X}{\setReal}{[0,1]}
          \separate
          F_X(x) \define P_X((-\infty,x])
        \]
        We call the function $\function{p}{\setReal}{[0,\infty)}$ a probability density of $X$ if for all $A\in\mathscr{B}(\setReal)$
        \[
          P_X(A) = \integral{A}{}{p(x)}{λ(x)}\ .
        \]
      \end{definition}

      % \begin{theorem}[Correspondence]
      %   Let $X$ be a real-valued random variable.
      %   There exists a unique monotone non-decreasing, right-continuous function $\function{F_X}{\setReal}{[0,1]}$ with
      %   \[
      %     \lim_{x\rightarrow -\infty} F_X(x) = 0
      %     \separate
      %     \lim_{x\rightarrow +\infty} F_X(x) = 1
      %   \]
      %   such that
      %   \[
      %     P_X((a,b]) = F(b) - F(a)
      %   \]
      % \end{theorem}

      As well as CDFs, probability densities can greatly simplify computations which are based on absolute continuous random variables.
      The following proposition, obtained from \textcite{schmidt2009}, shows the simplified computation of an expectation value through a Lebesgue integral.

      \begin{proposition}[Chaining]
        Let $X$ be a real-valued random variable with $p$ as its probability density.
        If $\function{f}{\setReal}{\setReal}$ is a measurable function such that $\expect \absolute{f\circ X} < \infty$ then
        \[
          \expect \roundBrackets{f\composition X} = \integral{\setReal}{}{f(x)p(x)}{λ(x)}\ .
        \]
      \end{proposition}

      A last important theorem to name is the strong law of large numbers (SLLN).
      According to \textcite[p.~13]{graham2013}, the principles of Monte Carlo methods are based on this theorem.
      Please note, there exist many more variations of this theorem.
      We will again use a simplified version from \textcite{graham2013}.

      \begin{theorem}[Strong Law of Large Numbers]
        Let $(X_n)_{n\in\setNatural}$ be a sequence of iid real-valued random variables with finite expectation value $μ$.
        Then the following equation holds $P$-almost everywhere.
        \[
          \lim_{n\to\infty} \frac{1}{n}\sum_{i=1}^n X_i = μ
        \]
      \end{theorem}
    % subsubsection stochastics (end)

    \subsubsection*{Finite Fields} % (fold)
    \label{ssub:finite_fields}

    % subsubsection finite_fields (end)
  % subsection mathematical_preliminaries (end)

  \subsection{Pseudorandom Number Generators} % (fold)
  \label{sub:pseudorandom_number_generators}

    % \subsubsection*{Random and Pseudorandom Sequences} % (fold)
    % \label{ssub:random_and_pseudorandom_sequences}
    In the above subsection \ref{sub:mathematical_preliminaries} the theory of probability was introduced to make an examination of randomness possible.
    Randomness is a difficult concept and drives many philosophical discussions.
    According to \textcite{volchan2002} and \textcite[\ppno~10-11]{kneusel2018}, humans have a bad intuition concerning the outcome of random experiments.
    But for our purposes, it would suffice to find a formal mathematical definition applicable to RNGs.
    However, such a formal concept, which is also widely accepted and unique, has not been found yet \autocite{volchan2002}.

    The first problem about randomness is the word itself.
    It is unclear and vague because there is no intentional application.
    To be more specific, we will observe randomness in form of random sequences of real numbers.
    But as stated in \textcite{volchan2002} the question if a sequence is random decides at infinity.
    Following his explanation, \textcite{volchan2002} makes clear that typical characterizations of a random sequence are closely associated with noncomputability.
    The question if a sequence is random cannot be algorithmically decided.
    Further, he proposed a more pragmatic principle: \enquote{if it acts randomly, it is random} --- the use of PRNGs.

    As a result, we will not use a formalized concept of randomness.
    A computer is only capable of using finite sequences of values and for the development of RNGs, it is enough to measure and compare different properties of truly random sequences to a sequence of real numbers.
    For this, we rely on probability theory and first define an abstract random sequence drawn from a random experiment.

    \begin{definition}[Random Sequence]
      Let $I$ be a countable index set and $(X_n)_{n\in I}$ be a sequence of iid real-valued random variables.
      Then a realization of $(X_n)_{n\in I}$ is called a random sequence.
    \end{definition}
    % subsubsection* random_and_pseudorandom_sequences (end)

    % \subsubsection*{Random and Pseudorandom Number Generators} % (fold)
    % \label{ssub:random_and_pseudorandom_number_generators}
    As stated above, the problem remains that it is not possible to generate a truly random sequence in a deterministic computer system.
    Therefore we will rely on so-called pseudorandom sequences.
    These are sequences of values generated by a PRNG.

    \begin{definition}[Pseudorandom Number Generator]
      A tuple $(S,s_0,T,U,G)$ is called a PRNG.
      $S$ is a non-empty, finite set of states.
      $s_0 \in S$ is the initial state.
      $\function{T}{S}{S}$ is the transition function.
      $U$ is a non-empty, finite set of output symbols.
      $\function{G}{S}{U}$ is the output function which generates an output symbol for every state.
    \end{definition}

    \begin{definition}[Pseudorandom Sequence of PRNG]
      $(s_n)_{n\in\setNatural}$ is the respective sequence of states
      \[
        s_{n+1} \define T(s_n)
      \]
      Pseudorandom sequence $(u_n)_{n\in\setNatural}$
      \[
        u_n = G(s_n)
      \]
    \end{definition}

    \begin{alignat*}{3}
      s_0 \xrightarrow{T} &s_1 \xrightarrow{T} &&s_2 \xrightarrow{T} &&\ldots \\
      G &\downarrow &&\downarrow \\
      &u_1 &&u_2 &&\ldots
    \end{alignat*}
    % subsubsection* random_and_pseudorandom_number_generators (end)
  % subsection pseudorandom_number_generators (end)

  \subsection{Simulation in Physics and Mathematics} % (fold)
  \label{sub:simulation_in_physics_and_mathematics}
    \subsubsection*{Mathematical and Physical Preliminaries} % (fold)
    \label{ssub:mathematical_and_physical_preliminaries}

    % subsubsection* mathematical_and_physical_preliminaries (end)

    \subsubsection*{Baseline Model Problems} % (fold)
    \label{ssub:baseline_model_problems}

    % subsubsection* baseline_model_problems (end)
  % subsection simulation_in_physics_and_mathematics (end)

  \subsection{SIMD-Capable Processors} % (fold)
  \label{sub:simd-capable_processors}
    \subsubsection*{Architecture of Modern Central Processing Units} % (fold)
    \label{ssub:architecture_of_modern_central_processing_units}

    % subsubsection* architecture_of_modern_central_processing_units (end)

    \subsubsection*{SIMD Instruction Sets and Efficiency} % (fold)
    \label{ssub:simd_instruction_sets_and_efficiency}

    % subsubsection* simd_instruction_sets_and_efficiency (end)

    \subsubsection*{SSE, AVX, AVX512} % (fold)
    \label{ssub:sse_avx_avx512}

    % subsubsection* sse_avx_avx512 (end)
  % subsection simd-capable_processors (end)

  \subsection{Summary} % (fold)
  \label{sub:summary}

    \textcite{volchan2002,kneusel2018}

  % subsection summary (end)
% section background (end)
\end{document}