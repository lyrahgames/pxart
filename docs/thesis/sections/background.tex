\documentclass{stdlocal}
\begin{document}
\section{Background} % (fold)
\label{sec:background}
  % virtual subsection Introduction
  To systematically approach the implementation of PRNGs, basic knowledge in the topics of stochastics and finite fields is administrable.
  Together, these topics will give a deeper understanding of randomness in deterministic computer systems, a formal description of pseudorandom sequences and generators, and the mathematical foundation of Monte Carlo algorithms.
  Based on them, we are capable of scientifically analyzing PRNGs concerning their randomness properties.
  Vectorization techniques can be conceptualized by the architecture of modern SIMD-capable multiprocessors and their instruction sets.
  Especially the knowledge of typical instructions will make the design of a new API and its application to Monte Carlo simulations clear.
  The following sections will give an overview of the named topics.

  \subsection{Mathematical Preliminaries} % (fold)
  \label{sub:mathematical_preliminaries}
    \subsubsection*{Probability Theory} % (fold)
    \label{ssub:stochastics}
      The observation of random experiments resulted in the construction of probability theory.
      But probability theory itself does not use a further formalized concept of randomness \autocite{schmidt2009}.
      % Randomness itself plays a minor role in probability theory and is used in form of realizations of random variables.
      In fact, it allows us to observe randomness without defining it \autocite{volchan2002}.
      % Actually, typical formalizations rely on probability theory.
      % This connection makes the development of RNGs possible.
      % Hence, in the following we will give only the formal definition of relevant structures without further discussions and will postpone an examination of randomness to the next subsection.
      Hence, we will postpone an examination of truly random sequences to the next section.

      According to \textcite{schmidt2009}, Kolmogorov embedded probability theory in the theory of measure and integration.
      Albeit it heavily relies on measure-theoretical structures, probability theory is one of the most important applications of measure and integration theory.
      Therefore we will assume basic knowledge in this topic and refer to \textcite{schmidt2009} and \textcite{elstrodt2011} for a more detailed introduction to measure spaces, measurable functions, and integrals.
      Propositions and theorems will be given without proof.

      The underlying structure of probability theory, which connects it to measure theory, is the probability space.
      It is a measure space with a finite and normalized measure.
      This gives access to all the usual results of measure theory and furthermore unifies discrete and continuous distributions.
      \autocite[p.~193~ff.]{schmidt2009}

      \begin{definition}[Probability Space]
        A probability space is a measure space $\roundBrackets{\Omega, \mathscr{F}, P}$ such that $P(\Omega)=1$.
        In this case, we call $P$ the probability measure, $\mathscr{F}$ the set of all events, and $\Omega$ the set of all possible outcomes of a random experiment.
      \end{definition}

      % For our purposes, the set of possible outcomes $\Omega$ will be a finite or countable infinite set.
      % Hence, we can choose $\mathscr{F}$ to be the power set $\mathscr{P}(\Omega)$.
      Due to the complex definition of a measure space, it is convenient to not have to explicitly specify the probability space when analyzing random experiments.
      Instead, we use random variables which are essentially measurable functions on a probability space \autocite[p.~194]{schmidt2009}.
      For complicated cases, these will serve as observables for specific properties and will make the analysis much more intuitive.

      \begin{definition}[Random Variable]
        Let $(\Omega,\mathscr{F},P)$ be a probability space and $(\Sigma,\mathscr{A})$ a measurable space.
        A measurable function $\function{X}{\Omega}{\Sigma}$ is called a random variable.

        In this case, we denote with $P_X\define P\composition\inverse{X}$ the distribution and with $(\Sigma,\mathscr{A},P_X)$ the probability space of $X$.
        % We call $X(ω)$ for $ω\in\Omega$ a realization of $X$.
        % Let $\function{Y}{\Omega}{\Sigma}$ be another random variable.
        % $X$ and $Y$ are identically distributed if $P_X = P_Y$.
        Two random variables are identically distributed if they have the same distribution.
        Additionally, we say that $X$ is a real-valued random variable if $\Sigma = \setReal$ and $\mathscr{A} = \mathscr{B}(\setReal)$.
      \end{definition}

      From now on, if a random variable is defined then, if not stated otherwise, it is assumed there exists a proper probability space $(\Omega,\mathscr{F},P)$ and measurable space $(\Sigma, \mathscr{A})$.

      Another important concept of stochastics is known as independence.
      In \textcite{schmidt2009} it is defined for a family of events, a family of sets of events, and a family of random variables.
      If we think of random variables as observables then their independence means that their outcomes do not influence each other.
      % It makes only sense in the context of probability theory
      For our purposes, the general definition of all three forms of independence is distracting.
      In a computer, it makes no sense to talk about infinite sequences.
      Therefore the following definition of independence takes only a finite sequence of random variables into account.
      Furthermore, to make it more understandable, this definition uses a theorem from \textcite[p.~238]{schmidt2009} which characterizes the independence of random variables.
      % Here, we will use a simpler equivalent definition  because, for a computer, all we need are finite sequences of random variables.

      \begin{definition}[Independence]
        % Let $(\Omega,\mathscr{F},P)$ be a probability space.
        % Two events $A, B \in \mathscr{F}$ are independent if $P(A\cap B)=P(A)P(B)$.
        % Let $(\Sigma_i,\mathscr{A}_i)$ for $i\in\set{1,2}{}$ be measurable spaces and $X_i$ random variables with $X\define X_1\times X_2$.
        % They are called independent if $P_X = P_{X_1} \otimes P_{X_2}$.
        Let $n\in\setNatural$ and $X_i$ be a random variable for all $i\in\setNatural$ with $i\leq n$.
        We denote the respective random vector with $X \define \roundBrackets{X_i}_{i=1}^n$.
        Then these random variables are independent if the following equation holds.
        \[
          P_X = \bigotimes_{i=1}^n P_{X_i}
        \]
      \end{definition}

      Typical observations of random sequences include the estimation of the expectation value and the variance.
      Both of these values are needed for analyzing PRNGs and the development of Monte Carlo simulations \autocite[p.~30~ff.]{landau2015}.
      Due to their deep connection to the integral, both of these moments are defined for real-valued random variables.
      We give the usual definitions based on \textcite[p.~274~ff.]{schmidt2009} in a simplified form.

      \begin{definition}[Expectation Value and Variance]
        Let $X$ be a real-valued random variable such that $\integral{\Omega}{}{\absolute{X}}{P}<\infty$.
        Then the expectation value $\expect X$ and variance $\var X$ of $X$ is defined in the following way.
        \[
          \expect X \define \integral{\Omega}{}{X(ω)}{P(ω)}
          \separate
          \var X \define \expect\roundBrackets{X - \expect X}^2
        \]
      \end{definition}

      To not rely on the underlying probability space directly, we want to be able to compute the expectation value through the respective distribution of the random variable.
      The theory of measure and integration gives the following proposition, also known as rule of substitution \autocite[p.~276]{schmidt2009}.

      \begin{proposition}[Substitution]
        Let $X$ be real-valued random variable and $\function{f}{\setReal}{\setReal}$ a measurable function such that $\integral{\Omega}{}{\absolute{f}}{P_X} < \infty$.
        Then the following equation holds.
        \[
          \expect(f\composition X) = \integral{\setReal}{}{f(x)}{P_X(x)}
        \]
        In particular, if $\expect \absolute{X} < \infty$ then the above equation can be reformulated as follows.
        \[
          \expect X = \integral{\setReal}{}{x}{P_X(x)}
        \]
      \end{proposition}

      The distribution of real-valued random variables is univariate and as a result can be described by so-called cumulative distribution functions (CDFs).
      The CDF intuitively characterizes the distribution and simplifies the analysis.
      Further, it can be proven that every CDF belongs to a univariate distribution.
      According to \textcite[p.~246]{schmidt2009}, this is the theorem of correspondence.
      Sometimes it is even possible to define a probability density; a function that is the Lebesgue density of the respective distribution \autocite[p.~255]{schmidt2009}.

      \begin{definition}[Probability Density and Cumulative Distribution Function]
        Let $X$ be a real-valued random variable.
        Then the respective cumulative distribution function is defined as follows.
        \[
          \function{F_X}{\setReal}{[0,1]}
          \separate
          F_X(x) \define P_X((-\infty,x])
        \]
        We call the function $\function{p}{\setReal}{[0,\infty)}$ a probability density of $X$ if for all $A\in\mathscr{B}(\setReal)$
        \[
          P_X(A) = \integral{A}{}{p(x)}{λ(x)}\ .
        \]
      \end{definition}

      % \begin{theorem}[Correspondence]
      %   Let $X$ be a real-valued random variable.
      %   There exists a unique monotone non-decreasing, right-continuous function $\function{F_X}{\setReal}{[0,1]}$ with
      %   \[
      %     \lim_{x\rightarrow -\infty} F_X(x) = 0
      %     \separate
      %     \lim_{x\rightarrow +\infty} F_X(x) = 1
      %   \]
      %   such that
      %   \[
      %     P_X((a,b]) = F(b) - F(a)
      %   \]
      % \end{theorem}

      As well as CDFs, probability densities can greatly simplify computations which are based on absolute continuous random variables.
      The following proposition, obtained from \textcite{schmidt2009}, shows the simplified computation of an expectation value through a Lebesgue integral.

      \begin{proposition}[Chaining]
        Let $X$ be a real-valued random variable with $p$ as its probability density.
        If $\function{f}{\setReal}{\setReal}$ is a measurable function such that $\expect \absolute{f\circ X} < \infty$ then
        \[
          \expect \roundBrackets{f\composition X} = \integral{\setReal}{}{f(x)p(x)}{λ(x)}\ .
        \]
      \end{proposition}

      A last important theorem to name is the strong law of large numbers (SLLN).
      According to \textcite[p.~13]{graham2013}, the principles of Monte Carlo methods are based on this theorem.
      Please note, there exist many more variations of this theorem.
      We will again use a simplified version from \textcite{graham2013}.

      \begin{theorem}[Strong Law of Large Numbers]
        Let $(X_n)_{n\in\setNatural}$ be a sequence of iid real-valued random variables with finite expectation value $μ$.
        Then the following equation holds $P$-almost everywhere.
        \[
          \lim_{n\to\infty} \frac{1}{n}\sum_{i=1}^n X_i = μ
        \]
      \end{theorem}
    % subsubsection stochastics (end)

    \subsubsection*{Finite Fields} % (fold)
    \label{ssub:finite_fields}

    % subsubsection finite_fields (end)
  % subsection mathematical_preliminaries (end)

  \subsection{Pseudorandom Number Generators} % (fold)
  \label{sub:pseudorandom_number_generators}

    % \subsubsection*{Random and Pseudorandom Sequences} % (fold)
    % \label{ssub:random_and_pseudorandom_sequences}
    \subsubsection*{Random Sequences}
    In the above subsection \ref{sub:mathematical_preliminaries} the theory of probability was introduced to make an examination of randomness possible.
    Randomness is a difficult concept and drives many philosophical discussions.
    According to \textcite{volchan2002} and \textcite[\ppno~10-11]{kneusel2018}, humans have a bad intuition concerning the outcome of random experiments.
    But for our purposes, it would suffice to find a formal mathematical definition applicable to RNGs.
    However, such a formal concept, which is also widely accepted and unique, has not been found yet \autocite{volchan2002}.

    The first problem about randomness is the word itself.
    It is unclear and vague because there is no intentional application.
    To be more specific, we will observe randomness in form of random sequences of real numbers.
    But as stated in \textcite{volchan2002} the question if a sequence is random decides at infinity.
    Following his explanation, \textcite{volchan2002} makes clear that typical characterizations of a random sequence are closely associated with noncomputability.
    The question if a sequence is random cannot be algorithmically decided.
    Further, he proposed a more pragmatic principle: \enquote{if it acts randomly, it is random} --- the use of PRNGs.

    As a result, we will not use a formalized concept of randomness.
    A computer is only capable of using finite sequences of values and for the development of RNGs, it is enough to measure and compare different properties of truly random sequences to a sequence of real numbers.
    For this, we rely on probability theory and first define an abstract random sequence drawn from a random experiment.

    \begin{definition}[Random Sequence]
      Let $I$ be a countable index set and $(X_n)_{n\in I}$ be a sequence of iid real-valued random variables.
      Then a realization of $(X_n)_{n\in I}$ is called a random sequence.
    \end{definition}
    % subsubsection* random_and_pseudorandom_sequences (end)

    % \subsubsection*{Random and Pseudorandom Number Generators} % (fold)
    % \label{ssub:random_and_pseudorandom_number_generators}
    \subsubsection*{Pseudorandom Sequences}
    Generating a truly random sequence in a deterministic computer system is impossible.
    But the given abstract definition of a random sequence in terms of probability theory helps to assess the randomness properties of a given sequence produced by a computer.
    Typically, a computer-generated sequence which fulfills various conditions about randomness will be called a pseudorandom sequence.
    For computer programming and simulations, the usage of a truly random sequence would even introduce severe disadvantages in contrast to a pseudorandom sequence.
    Concerning program verification, debugging, and the comparison of similar systems, the reproducibility of results is essential \autocite{lecuyer2015}.
    A truly random sequence produced by physical devices, such as thermal noise diodes or photon trajectory detectors, is not reproducible and can therefore not be conveniently used for mathematical and physical simulations \autocite{lecuyer2015}.
    According to \textcite{lecuyer2015}, a given simulation should produce the same results on different architectures for every run.
    This property becomes even more important if parallel generation of random numbers with multiple streams is taken into account.
    Additionally, considering the performance of random number generation PRNGs tend to be much faster than TRNGs.
    Thus, especially for Monte Carlo methods, PRNGs are a key resource for computer-generated random numbers \autocite{bauke2007}.

    For a detailed discussion about its mathematical properties, design, and implementation, the concept of a PRNG has to be formalized.
    In this thesis, we use the following slightly modified variation of \citeauthor{lecuyer1994}'s definition \autocite{lecuyer1994,lecuyer2015,barash2017,bauke2007}.
    It assumes a finite set of states and a transition function which advances the current state of the PRNG by a recurrence relation.
    For the output, a finite set of output symbols and a generator function which maps states to output symbols is chosen.
    As of \textcite{bauke2007}, almost all PRNGs produce a sequence of numbers by a recurrence.
    Hence, the given formalization is widely accepted and builds the basis for further discussions about pseudorandom numbers \autocite{lecuyer1994,lecuyer2015,barash2017,bauke2007}.

    \begin{definition}[Pseudorandom Number Generator]
      A tuple $(S,T,U,G)$ is called a PRNG.
      $S$ is a non-empty, finite set of states.
      $\function{T}{S}{S}$ is the transition function.
      $U$ is a non-empty, finite set of output symbols.
      $\function{G}{S}{U}$ is the output function which generates an output symbol for every state.
    \end{definition}

    Given a PRNG and a seed value as an initial state, producing a sequence of pseudorandom numbers can be done by periodically applying the transition function on the current state and then extracting the output through the generator function \autocite{barash2017,lecuyer1994,lecuyer2015}.
    Here, we will use this method as the generalization of a pseudorandom sequence.
    Figure \ref{fig:scheme-pseudorandom-sequence} shows this process schematically.

    \begin{definition}[Pseudorandom Sequence of PRNG]
      Let $\mathscr{G}\define (S,T,U,G)$ be a PRNG and $s_0\in S$ be the initial state, also called the seed value.
      The respective sequence of states $(s_n)_{n\in\setNatural}$ is given by the following equation for all $n\in\setNatural$.
      \[
        s_{n+1} \define T(s_n)
      \]
      The sequence $(u_n)_{n\in\setNatural}$ given by the following expression for all $n\in\setNatural$ is then called the respective pseudorandom sequence of $\mathscr{G}$ with seed $s_0$.
      \[
        u_n = G(s_n)
      \]
    \end{definition}

    \begin{figure}
      \center
      \begin{minipage}[b]{0.5\textwidth}
      \begin{alignat*}{3}
        s_0 \xrightarrow{T} &s_1 \xrightarrow{T} &&s_2 \xrightarrow{T} &&\ldots \\
        G &\downarrow &&\downarrow \\
        &u_1 &&u_2 &&\ldots
      \end{alignat*}
      \end{minipage}
      \caption[Generation of a Pseudorandom Sequence]{%
        The figure shows a scheme about the generation of a pseudorandom sequence for a given PRNG $\mathscr{G}\define (S,T,U,G)$ and seed value $s_0\in S$.
        The internal state is advanced by the transition function $T$ through a recurrence relation.
        To get an output value for the pseudorandom sequence the generator function $G$ is used.
      }
      \label{fig:scheme-pseudorandom-sequence}
    \end{figure}

    In the above definition, the initial state was deterministically given.
    As discussed, this behavior is intended to be able to reproduce the complete pseudorandom sequence based on its initial state.
    But to extend this process with true randomness, in general the seed will be chosen to be a truly random number generated by some physical device.
    \textcite{lecuyer1994} states that generating such a seed is much less work and more reasonable than generating a long sequence of truly random values.
    A generator with a truly random seed can be seen as an extensor of randomness.
    % subsubsection* random_and_pseudorandom_number_generators (end)

    \subsubsection*{Linear Congruential Generator} % (fold)
    \label{ssub:linear_congruential_generator}
      \begin{definition}[Linear Congruential Generator]
        Let $m\in\setNatural$ with $m\geq 2$ and $a,b\in\setInteger_m$.
        We define the PRNG $\mathrm{LCG}(m,a,b) \define (S,T,U,G)$
        \[
          S \define U \define \setInteger_m
          \separate
          G \define \identity_{\setInteger_m}
        \]
        \[
          \function{T}{S}{S}
          \separate
          T(x) \define a x + c
        \]
        Multiplication and addition are understood in the sense of $\setInteger_m$.
        We call $\mathrm{LCG}(m,a,b)$ the linear congruential generator with modulus $m$, multiplier $a$ and increment $c$.
      \end{definition}
    % subsubsection linear_congruential_generator (end)

    \subsubsection*{Mersenne Twister} % (fold)
    \label{ssub:mersenne_twister}
      \begin{definition}[Mersenne Twister]
        Let $w, n, m\in \setNatural$ and $r\in\setNatural_0$ with $m \leq n$ and $r < w$.
        Further, let $a,b,c \in \setInteger_2^w$ and $u,s,t,l \in \setInteger_w$.
        Then the Mersenne Twister $\mathrm{MT}(w,n,m,r,a,b,c,u,s,t,l)\define (S,T,U,G)$ is defined as a PRNG in the following way.
        \[
          S \define \setInteger_n \times \setInteger_2^{w\times n}
          \separate
          U \define \setInteger_2^w
        \]
        \[
          \function{T}{S}{S}
        \]
        \[
          \forall i\in\setInteger_{n-1}: T(i, x) \define (i + 1, x)
        \]
        \[
          T(n-1, x) \define (0, y)
        \]
        \begin{align*}
          \forall i\in\setInteger_{n-m}: y_i &\define x_{m+i} \oplus \roundBrackets{x_i^u \middle\vert x_{i+1}^l}A \\
          \forall i\in\setInteger_{m-1} + (n-m): y_i &\define y_{i-(n-m)} \oplus \roundBrackets{x_i^u \middle\vert x_{i+1}^l}A \\
          y_{n-1} &\define y_{m-1} \oplus \roundBrackets{x^u_{n-1} \middle\vert y_0^l}A
        \end{align*}
        \[
          xA \define
          \begin{cases}
            x \gg 1 &: x_0 = 0 \\
            (x \gg 1) \oplus a &: x_0 = 1
          \end{cases}
        \]
        \[
          f_1(x) \define x \oplus (x \gg u)
        \]
        \[
          f_2(x) \define x \oplus ((x \ll s) \odot b)
        \]
        \[
          f_3(x) \define x \oplus ((x \ll t) \odot c)
        \]
        \[
          f_4(x) \define x \oplus (x \gg l)
        \]
        \[
          \function{G}{S}{U}
          \separate
          G(i,x) \define f_4 \circ f_3 \circ f_2 \circ f_1(x_i)
        \]
      \end{definition}
    % subsubsection mersenne_twister (end)

    \subsubsection*{Permuted Congruential Generator} % (fold)
    \label{ssub:permuted_congruential_generator}

    % subsubsection permuted_congruential_generator (end)

    \subsubsection*{Xorshift and Variants} % (fold)
    \label{ssub:xorshift_and_variants}

    % subsubsection xorshift_and_variants (end)

    \subsubsection*{Middle Square Weyl Sequence RNG} % (fold)
    \label{ssub:middle_square_weyl_sequence_rng}
      \begin{definition}[Middle Square Weyl Sequence RNG]
        Let $s\in\setInteger_{2^{64}}$ be an odd constant.
        The middle square Weyl sequence RNG $\mathrm{MSWS}(s)\define(S,T,U,G)$ is defined as a PRNG in the following way.
        \[
          S \define \setInteger_{2^{64}}^2
          \separate
          U \define \setInteger_{2^{32}}
        \]
        \[
          \function{T}{S}{S}
          \separate
          T(w,x) = (w+s, f(x^2 + w + s))
        \]
        \[
          \function{f}{\setInteger_{2^{64}}}{\setInteger_{2^{64}}}
          \separate
          f(x) \define (x \gg 32) \mathrm{or} (x \ll 32)
        \]
        \[
          \function{G}{S}{U}
          \separate
          G(w,x) \define x \mod 2^{32}
        \]
      \end{definition}
    % subsubsection middle_square_weyl_sequence_rng (end)
  % subsection pseudorandom_number_generators (end)

  \subsection{Simulation in Physics and Mathematics} % (fold)
  \label{sub:simulation_in_physics_and_mathematics}
    \subsubsection*{Mathematical and Physical Preliminaries} % (fold)
    \label{ssub:mathematical_and_physical_preliminaries}

    % subsubsection* mathematical_and_physical_preliminaries (end)

    \subsubsection*{Baseline Model Problems} % (fold)
    \label{ssub:baseline_model_problems}

    % subsubsection* baseline_model_problems (end)
  % subsection simulation_in_physics_and_mathematics (end)

  \subsection{SIMD-Capable Processors} % (fold)
  \label{sub:simd-capable_processors}
    \subsubsection*{Architecture of Modern Central Processing Units} % (fold)
    \label{ssub:architecture_of_modern_central_processing_units}

    % subsubsection* architecture_of_modern_central_processing_units (end)

    \subsubsection*{SIMD Instruction Sets and Efficiency} % (fold)
    \label{ssub:simd_instruction_sets_and_efficiency}

    % subsubsection* simd_instruction_sets_and_efficiency (end)

    \subsubsection*{SSE, AVX, AVX512} % (fold)
    \label{ssub:sse_avx_avx512}

    % subsubsection* sse_avx_avx512 (end)
  % subsection simd-capable_processors (end)

  \subsection{Summary} % (fold)
  \label{sub:summary}

    \textcite{volchan2002,kneusel2018}

  % subsection summary (end)
% section background (end)
\end{document}